# Initial Post: The Risks and Benefits of AI Writers

*Posted by Fabian Narel - Tuesday, 8 April 2025, 1:55 AM*

Large Language Models (LLMs) like GPT-3 have become wildly popular for their human-like writing ability and they're now used for everything from business memos to creative fiction (Hutson, 2021), however it comes at a price of both risks and benefits.

In low-risk administrative tasks, LLMs are mostly beneficial with minimal downside, they can draft a polite email or a meeting summary in seconds (Hutson, 2021), saving time and helping with writer's block. Any minor errors or awkward phrasing are easy for a human to catch and fix, so the overall risk stays low.

In medium-risk professional writing (marketing, journalism, etc.), LLMs offer speed but need caution. They can generate a solid press release or news blurb in a flash, yet over-reliance can backfire. For example, when the tech site CNET used an AI to write finance articles, it later issued corrections for numerous factual errors (Farhi, 2023). Here the benefit of a quick drafts is balanced by the risk of inaccuracies or bland, boilerplate prose. Human editors must stay in the loop to fact-check and refine the output.

For higher-risk creative writing such as fiction, poetry, and the like the stakes are highest. LLMs can mimic an author's style and pour out pages of text, but their originality suffers. The AI's stories often feel derivative because it's remixing past works rather than truly inventing (Wenger and Kenett, 2023). This also raises plagiarism worries; a model might inadvertently paste chunks of its training data (Tang, 2023). Moreover, because it's so easy to generate text, there's a risk of overproduction a flood of formulaic AI content that could overwhelm readers and devalue genuine creativity. In these nuanced tasks, LLMs make intriguing collaborators, but they can't replace the originality and judgment of a human writer.

Another layer of risk when using LLMs comes from their tendency to be agreeable but unreliable (Goetz et al., 2023). These models are built to please the user, not to challenge them. That might be fine when drafting an email, but in something like ethics education, where students are supposed to develop critical thinking, it's a real problem. Instead of encouraging students to think deeply, LLMs might just echo back the kind of answer the user wants to hear.

As Coeckelbergh (2025) points out, these models are optimized to produce plausible-sounding text, not necessarily true statements. That means even when they're wrong, they sound right. This is dangerous, especially in political or educational settings, where people may trust the output without questioning it as it happens often with other phenomenas like fake news.

## References

Coeckelbergh, M. (2025) Truth, post-truth, and democracy in the age of artificial intelligence: Epistemological and political risks of language models. AI and Society. Available from: https://doi.org/10.1007/s00148-025-00529-0 [Accessed 7 April 2025].

Farhi, P. (2023) A news site used AI to write articles. It was a journalistic disaster. The Washington Post, 17 January. Available from: https://www.washingtonpost.com/media/2023/01/17/cnet-ai-articles-journalism-corrections/ [Accessed 5 April 2025].

Goetz, L., Trengove, M., Trotsyuk, A. and Federico, C.A. (2023) Unreliable LLM bioethics assistants: Ethical and pedagogical risks. The American Journal of Bioethics, 23(10), pp.89–91. Available from: https://doi.org/10.1080/15265161.2023.2249843 [Accessed 7 April 2025].

Hutson, M. (2021) Robo-writers: the rise and risks of language-generating AI. Nature 591(7848): 22–25.

Tang, B. L. (2023) The underappreciated wrong of AIgiarism – bypass plagiarism that risks propagation of erroneous and bias content. EXCLI Journal 22: 907–910. DOI: 10.17179/excli2023-6435.

Wenger, E. and Kenett, Y. (2023) We're Different, We're the Same: Creative Homogeneity Across LLMs. arXiv preprint arXiv:2304.00008. 